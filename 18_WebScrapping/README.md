# üìó Lecci√≥n 18: Web Scrapping

**[√çndice](../README.md)**

**[Anterior](../17_HTTP/README.md)**

## Web Scrapping

**Web scrapping** es el conjunto de t√©cnicas utilizadas para extraer datos de p√°ginas web. En definitiva de simular la visita a una p√°gina web por un navegador, extraer informaci√≥n de la web y hacerla accesible de forma estructurada. El web scrapping es √∫til cuando se necesita extraer informaci√≥n/funcionalidad de forma autom√°tica de una web que no tiene una API.

> ‚ö† **AVISO:** El web scrapping puede ser ilegal y no estar permitido por los t√©rminos y condiciones de algunas p√°ginas web. Como m√≠nimo se deber√≠a consultar el robots.txt de la p√°gina web objetivo para comprobar las limitaciones al web scrapping.

Aunque hay otras, la librer√≠a de python m√°s conocida para hacer Web Scrapping es (Beautiful Soup)[https://www.crummy.com/software/BeautifulSoup/bs4/doc/]. Esta librer√≠a se utiliza en combinaci√≥n con *requests*^.

Para instalarla:

```python
pip install beautifulsoup4
```

## Paso a paso

De forma general, los pasos a dar para hacer Web Scrapping son:

### 1. Identificar los elementos de la web

Las webs son documentos estructurados en HTML. Los elementos de la web se estructuran y definen mediante etiquetas HTML. Cada etiqueta contiene instrucciones sencillas que indican al navegador c√≥mo dar formato al texto y a definir los diversos elementos de la p√°gina web. Al aplicar estas etiquetas de marcado a los diferentes elementos del texto, se indica al navegador c√≥mo mostrarlos al usuario, lo que permite crear p√°ginas web estructuradas y con un dise√±o coherente.

El primer paso es identificar qu√© elementos de la web contienen la informaci√≥n/datos que se quieren extraer. Para ello se visita la web con un navegador y se inspecciona el c√≥digo hasta identificar los elementos HTML. De cada uno de los elementos es interesante anotar la *etiqueta HTML*, la *clase* y/o el *id*.

### 2. Identificar las peticiones que permiten descargar la p√°gina

Es necesario identificar qu√© peticiones HTTP ser√° necesario replicar con *requests* para que el servidor devuelva la p√°gina HTML de la que se desea extraer informaci√≥n. Es interante identificar la url de destino, par√°metros en url, el m√©todo HTTP (GET, POST, ...), las cabeceras (user-agent, ...) y datos necesarios para que el servidor devuelva la p√°gina HTML adecuada.

Para ello, la mejor herramienta es de nuevo el navegador y las herramientas de desarrollo.

### 3. Replicar la petici√≥n con *requests*

Ya en Python se utilizar√° *requests* para replicar la petici√≥n HTTP necesaria:

```python
import requests
from bs4 import BeautifulSoup

headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Cookie': 'cookie-agreed=0',
        'Upgrade-Insecure-Requests': '1'
    }

url = ''
req = requests.get(url, headers=headers)
print(req.url)
if req.status_code == 200:
    html = BeautifulSoup(req.text, 'html.parser') # Beautiful Soup para parsear el html y poder buscar elementos
```

### 4. Buscar y parsear con *Beautiful Soup* los elementos identificados

Con *BeautifulSoup* ahora es necesario localizar los elementos interesantes y parsear el contenido:
```python
resoluciones = html.find_all('div', {'class': 'layout__region--content'})
    for resolucion in resoluciones:
        titulo = ''
        link = ''
        fecha = ''
        tag_titulo = resolucion.find('div', {'class': 'field--name-title'})
        if tag_titulo:
            titulo = tag_titulo.getText()
        tag_link = resolucion.find('a',{'class': ''})
        print(tag_link)
        if tag_link:
            link = '' + tag_link['href']
        tag_fecha = resolucion.find('time')
        if tag_fecha:
            fecha = tag_fecha['datetime']
        print((titulo,link,fecha))
```

Si fuera, necesario podr√≠an descargarse todos los enlaces:

```python
import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urlparse


headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Cookie': 'cookie-agreed=0',
        'Upgrade-Insecure-Requests': '1'
    }


def download(url, filepath):
    with requests.get(url, stream = True, timeout=200) as r:
        with open(filepath, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                f.write(chunk)



url = '' # TODO: Rellenar url objetivo
parsed_url = urlparse(url)
req = requests.get(url, headers=headers, timeout=200)
if req.status_code == 200:
    html = BeautifulSoup(req.text, 'html.parser') # Beautiful Soup para parsear el html y poder buscar elementos
    with open('./datos.csv', mode='wt', encoding='utf-8') as f:
        csv_file = csv.writer(f, delimiter = ',')
        csv_file.writerow(['titulo', 'enlace', 'fecha'])
        resoluciones = html.find_all('div', {'class': 'layout__region--content'})
        for resolucion in resoluciones:
            titulo = ''
            link = ''
            fecha = ''
            tag_titulo = resolucion.find('div', {'class': 'field--name-title'})
            if tag_titulo:
                titulo = tag_titulo.getText()
            tag_link = resolucion.find('a',{'class': ''})
            if tag_link:
                link = f'{parsed_url.scheme}://{parsed_url.hostname}/' + tag_link['href']
            tag_fecha = resolucion.find('time')
            if tag_fecha:
                fecha = tag_fecha['datetime']
            if all([titulo,link,fecha]):
                csv_file.writerow([titulo, link, fecha])
                download(link, f"{titulo}.pdf")
```

## Proyecto de Ejemplo: Obtener los datos de una empresa a partir del NIF con WebScrapping

El **objetivo del proyecto** es crear un **m√≥dulo de python** con una funci√≥n que pueda reutilizarse desde otros proyectos y que el m√≥dulo pueda llamarse por s√≠ mismo para obtener los datos de una empresa a partir de su NIF.

El m√≥dulo se llamar√° empresas.py y contendr√° una √∫nica funcion:

1. Una funci√≥n *get*. Recibe como par√°metro de entrada una lista de NIFs a buscar. Devuelve un diccionario/json con los datos de las empresas que haya localizado. 

El m√≥dulo se podr√° llamar desde l√≠nea de comando especificando tantos NIFs como se quieran: NIF1 NIF2 NIF3 ...

Soluci√≥n: Encuentra una posible soluci√≥n en (empresas)[https://github.com/mercaderd/curso_python/tree/main/18_WebScrapping/empresas]

**[Siguiente](../19_/Objetos.md)**
